{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dwiahmad/03-autograd?scriptVersionId=289956363\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 03 - Autograd & Gradient\n\n**PyTorch Course by Dwi Ahmad Dzulhijjah**\n\n---\n\n## üìå Learning Objectives\n\nPada notebook ini, kamu akan belajar:\n- Konsep automatic differentiation\n- Cara kerja `requires_grad` dan `backward()`\n- Computational graph di PyTorch\n- Gradient accumulation dan cara reset gradient\n- Detach tensor dari computation graph","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:54:40.669382Z","iopub.execute_input":"2026-01-04T08:54:40.669698Z","iopub.status.idle":"2026-01-04T08:54:46.328286Z","shell.execute_reply.started":"2026-01-04T08:54:40.669669Z","shell.execute_reply":"2026-01-04T08:54:46.327371Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## 1. Apa itu Autograd?\n\n**Autograd** adalah engine automatic differentiation di PyTorch. Autograd secara otomatis menghitung gradient dari semua operasi pada tensor.\n\nIni adalah fondasi dari training neural network menggunakan **backpropagation**.\n\n### Konsep Dasar:\n- **Forward pass**: Hitung output dari input\n- **Backward pass**: Hitung gradient loss terhadap setiap parameter","metadata":{}},{"cell_type":"markdown","source":"## 2. requires_grad\n\nUntuk mengaktifkan tracking gradient pada tensor, set `requires_grad=True`.","metadata":{}},{"cell_type":"code","source":"# Tensor tanpa gradient tracking\nx = torch.tensor([1.0, 2.0, 3.0])\nprint(f\"x requires_grad: {x.requires_grad}\")\n\n# Tensor dengan gradient tracking\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\nprint(f\"x requires_grad: {x.requires_grad}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:54:46.361561Z","iopub.execute_input":"2026-01-04T08:54:46.361852Z","iopub.status.idle":"2026-01-04T08:54:46.36799Z","shell.execute_reply.started":"2026-01-04T08:54:46.361816Z","shell.execute_reply":"2026-01-04T08:54:46.367125Z"}},"outputs":[{"name":"stdout","text":"x requires_grad: False\nx requires_grad: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Mengaktifkan requires_grad setelah tensor dibuat\nx = torch.tensor([1.0, 2.0, 3.0])\nx.requires_grad_(True)  # in-place operation\nprint(f\"x requires_grad: {x.requires_grad}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:54:59.548328Z","iopub.execute_input":"2026-01-04T08:54:59.548818Z","iopub.status.idle":"2026-01-04T08:54:59.557575Z","shell.execute_reply.started":"2026-01-04T08:54:59.548781Z","shell.execute_reply":"2026-01-04T08:54:59.55612Z"}},"outputs":[{"name":"stdout","text":"x requires_grad: True\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Computational Graph\n\nKetika melakukan operasi pada tensor dengan `requires_grad=True`, PyTorch membangun **computational graph** yang merekam semua operasi.","metadata":{}},{"cell_type":"code","source":"# Contoh sederhana: y = x^2\nx = torch.tensor(3.0, requires_grad=True)\ny = x ** 2\n\nprint(f\"x = {x}\")\nprint(f\"y = x^2 = {y}\")\nprint(f\"y.grad_fn = {y.grad_fn}\")  # Function yang membuat y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:55:13.965461Z","iopub.execute_input":"2026-01-04T08:55:13.966717Z","iopub.status.idle":"2026-01-04T08:55:14.005059Z","shell.execute_reply.started":"2026-01-04T08:55:13.966663Z","shell.execute_reply":"2026-01-04T08:55:14.003928Z"}},"outputs":[{"name":"stdout","text":"x = 3.0\ny = x^2 = 9.0\ny.grad_fn = <PowBackward0 object at 0x7d4807e33df0>\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 4. Backward() - Menghitung Gradient\n\nFungsi `backward()` menghitung gradient dari output terhadap semua input yang memiliki `requires_grad=True`.","metadata":{}},{"cell_type":"code","source":"# y = x^2\n# dy/dx = 2x\n# Jika x = 3, maka dy/dx = 6\n\nx = torch.tensor(3.0, requires_grad=True)\ny = x ** 2\n\n# Hitung gradient\ny.backward()\n\nprint(f\"x = {x}\")\nprint(f\"y = x^2 = {y}\")\nprint(f\"dy/dx = {x.grad}\")  # Seharusnya 2 * 3 = 6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:55:21.338058Z","iopub.execute_input":"2026-01-04T08:55:21.338462Z","iopub.status.idle":"2026-01-04T08:55:21.378559Z","shell.execute_reply.started":"2026-01-04T08:55:21.33843Z","shell.execute_reply":"2026-01-04T08:55:21.377486Z"}},"outputs":[{"name":"stdout","text":"x = 3.0\ny = x^2 = 9.0\ndy/dx = 6.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Contoh lebih kompleks: z = (x + y) * y\n# dz/dx = y\n# dz/dy = x + 2y\n\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(3.0, requires_grad=True)\n\nz = (x + y) * y  # z = xy + y^2\n\nz.backward()\n\nprint(f\"x = {x}, y = {y}\")\nprint(f\"z = (x + y) * y = {z}\")\nprint(f\"dz/dx = {x.grad}\")  # y = 3\nprint(f\"dz/dy = {y.grad}\")  # x + 2y = 2 + 6 = 8","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Gradient pada Vector/Matrix\n\nUntuk tensor multi-dimensi, kita perlu memberikan `gradient` argument ke `backward()`.","metadata":{}},{"cell_type":"code","source":"# Untuk scalar output, langsung backward()\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = x ** 2\nz = y.sum()  # Reduce ke scalar\n\nz.backward()\nprint(f\"x = {x}\")\nprint(f\"y = x^2 = {y}\")\nprint(f\"z = sum(y) = {z}\")\nprint(f\"dz/dx = {x.grad}\")  # [2, 4, 6]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Untuk vector output, perlu gradient argument\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = x ** 2\n\n# Vector-Jacobian product\nv = torch.tensor([1.0, 1.0, 1.0])  # \"upstream gradient\"\ny.backward(v)\n\nprint(f\"x = {x}\")\nprint(f\"y = x^2 = {y}\")\nprint(f\"dy/dx = {x.grad}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Gradient Accumulation\n\n‚ö†Ô∏è **Penting**: Gradient di PyTorch **terakumulasi** secara default! Kamu harus **reset gradient** sebelum setiap backward pass.","metadata":{}},{"cell_type":"code","source":"# Contoh accumulation (salah)\nx = torch.tensor(3.0, requires_grad=True)\n\nfor i in range(3):\n    y = x ** 2\n    y.backward()\n    print(f\"Iteration {i+1}: x.grad = {x.grad}\")\n    # Gradient terus bertambah: 6, 12, 18...","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cara yang benar: reset gradient\nx = torch.tensor(3.0, requires_grad=True)\n\nfor i in range(3):\n    y = x ** 2\n    y.backward()\n    print(f\"Iteration {i+1}: x.grad = {x.grad}\")\n    \n    # Reset gradient!\n    x.grad.zero_()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Detach & No Grad\n\nKadang kita perlu menghentikan gradient tracking untuk efisiensi atau keperluan tertentu.","metadata":{}},{"cell_type":"code","source":"# Menggunakan torch.no_grad()\nx = torch.tensor(3.0, requires_grad=True)\n\n# Dengan gradient tracking\ny = x ** 2\nprint(f\"y requires_grad: {y.requires_grad}\")\n\n# Tanpa gradient tracking\nwith torch.no_grad():\n    z = x ** 2\n    print(f\"z requires_grad: {z.requires_grad}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Menggunakan detach()\nx = torch.tensor(3.0, requires_grad=True)\ny = x ** 2\n\n# Detach dari computation graph\ny_detached = y.detach()\n\nprint(f\"y requires_grad: {y.requires_grad}\")\nprint(f\"y_detached requires_grad: {y_detached.requires_grad}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Practical Example: Linear Regression Manual\n\nMari implementasi linear regression sederhana menggunakan autograd!","metadata":{}},{"cell_type":"code","source":"# Generate synthetic data\n# y = 2x + 1 + noise\ntorch.manual_seed(42)\n\nX = torch.linspace(0, 10, 100).unsqueeze(1)\ny_true = 2 * X + 1 + torch.randn(100, 1) * 0.5\n\nplt.figure(figsize=(8, 5))\nplt.scatter(X.numpy(), y_true.numpy(), alpha=0.6)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Synthetic Data: y = 2x + 1 + noise')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize parameters\nw = torch.tensor(0.0, requires_grad=True)  # weight\nb = torch.tensor(0.0, requires_grad=True)  # bias\n\n# Hyperparameters\nlearning_rate = 0.01\nepochs = 100\n\n# Training loop\nlosses = []\n\nfor epoch in range(epochs):\n    # Forward pass: y_pred = w * x + b\n    y_pred = w * X + b\n    \n    # Compute loss (MSE)\n    loss = ((y_pred - y_true) ** 2).mean()\n    losses.append(loss.item())\n    \n    # Backward pass\n    loss.backward()\n    \n    # Update parameters (gradient descent)\n    with torch.no_grad():\n        w -= learning_rate * w.grad\n        b -= learning_rate * b.grad\n    \n    # Reset gradients\n    w.grad.zero_()\n    b.grad.zero_()\n    \n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, w = {w.item():.4f}, b = {b.item():.4f}\")\n\nprint(f\"\\nFinal: w = {w.item():.4f}, b = {b.item():.4f}\")\nprint(f\"True values: w = 2.0, b = 1.0\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot results\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss curve\naxes[0].plot(losses)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss')\n\n# Prediction\nwith torch.no_grad():\n    y_pred = w * X + b\n\naxes[1].scatter(X.numpy(), y_true.numpy(), alpha=0.6, label='Data')\naxes[1].plot(X.numpy(), y_pred.numpy(), 'r-', linewidth=2, label=f'Prediction: y = {w.item():.2f}x + {b.item():.2f}')\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('y')\naxes[1].set_title('Linear Regression Result')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Visualisasi Gradient\n\nMari visualisasikan bagaimana gradient berubah pada berbagai nilai x.","metadata":{}},{"cell_type":"code","source":"# Visualisasi gradient dari y = x^2\nx_vals = torch.linspace(-5, 5, 100)\ny_vals = []\ngrad_vals = []\n\nfor x_val in x_vals:\n    x = torch.tensor(x_val.item(), requires_grad=True)\n    y = x ** 2\n    y.backward()\n    \n    y_vals.append(y.item())\n    grad_vals.append(x.grad.item())\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(x_vals.numpy(), y_vals)\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\naxes[0].set_title('y = x¬≤')\naxes[0].grid(True)\n\naxes[1].plot(x_vals.numpy(), grad_vals, color='orange')\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('dy/dx')\naxes[1].set_title('Gradient: dy/dx = 2x')\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìù Exercises\n\n1. Hitung gradient dari fungsi `f(x) = x¬≥ + 2x¬≤ - 5x + 3` pada x = 2\n2. Implementasi polynomial regression menggunakan autograd\n3. Buat visualisasi gradient dari fungsi `f(x) = sin(x)`","metadata":{}},{"cell_type":"code","source":"# Exercise 1: f(x) = x¬≥ + 2x¬≤ - 5x + 3\n# f'(x) = 3x¬≤ + 4x - 5\n# f'(2) = 3(4) + 4(2) - 5 = 12 + 8 - 5 = 15\n\n# Your code here\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Exercise 2: Polynomial regression\n# Your code here\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Exercise 3: Visualisasi gradient sin(x)\n# Your code here\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéØ Summary\n\n| Konsep | Penjelasan |\n|--------|------------|\n| `requires_grad=True` | Aktifkan tracking gradient |\n| `backward()` | Hitung gradient (backpropagation) |\n| `.grad` | Akses gradient yang sudah dihitung |\n| `.grad.zero_()` | Reset gradient ke 0 |\n| `torch.no_grad()` | Disable gradient tracking |\n| `.detach()` | Lepaskan tensor dari computation graph |\n\n---\n\n**Selesai Modul 01! üéâ**\n\n**Next:** [02 - Neural Networks](../02_Neural_Networks/01_linear_layers.ipynb)","metadata":{}}]}